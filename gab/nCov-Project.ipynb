{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jrdaos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from dateutil import parser\n",
    "import re\n",
    "from geotext import GeoText\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "import geopandas as gpd\n",
    "\n",
    "# Spacy for tokenizing our texts\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Gensim is needed for modeling\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Spacy Tokenizer\n",
    "nlp = English()\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# This will add pipelines in our tokenization process.\n",
    "\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is a function that will create a model that predicts the topics conveyed by each group of tweet sentiments\n",
    "\n",
    "\n",
    "def topic_modeler(tokenized_texts, no_topics, no_words):\n",
    "    topics = []\n",
    "\n",
    "    words = corpora.Dictionary(tokenized_texts)\n",
    "    corpus = [words.doc2bow(doc) for doc in tokenized_texts]\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=words,\n",
    "                                                random_state = 3,\n",
    "                                               num_topics= no_topics)\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_texts, dictionary=words, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rappler_scraping.csv')\n",
    "df = df.iloc[:,1:]\n",
    "df['date'] = [parser.parse(date).strftime('%Y-%m-%d') for date in df['date']]\n",
    "df = df[(df['text'].str.contains('coronavirus'))]\n",
    "df = df[df['category'] == 'Philippines']\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "location = pd.read_csv('ph_locations.csv')\n",
    "location = location.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Topics \n",
    "\n",
    "words = df['text'].str.lower()\n",
    "listWords = []\n",
    "for item in words:\n",
    "    listWords.append([nlp(item)])\n",
    "\n",
    "topics = []\n",
    "for x in listWords:\n",
    "    res = topic_modeler(x, 1, 30)\n",
    "    res = res.show_topic(0, topn = 30)\n",
    "    topics.append([word[0] for word in res])\n",
    "    \n",
    "df['LDA_Topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the counting phrases in the articles\n",
    "\n",
    "df['count_docs'] =  df['text'].apply(lambda x: re.findall(\"\\d+(?:,\\d+)?\\s+[a-zA-Z]+\", x))\n",
    "\n",
    "checker = ['confirmed','suspected','quarantine','case','infected','monitoring','chinese','monitored']\n",
    "\n",
    "count_docs = []\n",
    "for index, row in df.iterrows():\n",
    "    passed = []\n",
    "    for item in row['count_docs']:\n",
    "        if any(ext in item.lower() for ext in checker):\n",
    "            passed.append(item)\n",
    "            break\n",
    "    \n",
    "    count_docs.append(passed)\n",
    "\n",
    "df['count_docs'] = count_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the PH Locations using geotext on the articles\n",
    "\n",
    "df['PH_Loc'] = [list(set(GeoText(content, 'PH').cities)) for content in df['text']]\n",
    "df['PH_Loc'] = [[x.lower() for x in w] for w in df['PH_Loc']]\n",
    "df['PH_Loc'] =[[x.replace('city', '') for x in w] for w in df['PH_Loc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying which articles are about suspicious or confirmed cases of the virus\n",
    "\n",
    "status = []\n",
    "for index, row in df.iterrows():\n",
    "    if ('confirmed' in row['LDA_Topics']) & ('confirm' in row['title'])  & (row['date'] >= '2020-01-30'):\n",
    "        status.append('confirmed')\n",
    "    elif ('confirmed' in row['LDA_Topics']) & (row['date'] >= '2020-01-30'):\n",
    "        status.append('confirmed')\n",
    "    elif (any(words in row['LDA_Topics']  for words in ['suspected','quarantine','case','infected','monitoring']))& ('FACT CHECK' not in row['title']) & ('FALSE' not in row['title']):\n",
    "        status.append('suspected')\n",
    "    else:\n",
    "        status.append('')\n",
    "df['status'] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Provinces in the identified locations\n",
    "\n",
    "df['PH_Loc'] = [list(set(loc) & set(location['Pro_Name'].unique())) for loc in df['PH_Loc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For locations not identified through the text, it will check with the LDA topics if a location is identified and use it instead\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if len(row['PH_Loc']) == 0:\n",
    "        try:\n",
    "            df.loc[index, 'PH_Loc'] = [list(set(row['LDA_Topics']) & set(location['Pro_Name'].unique()))]\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the document counts to just numbers\n",
    "\n",
    "counts = []\n",
    "case = []\n",
    "for count in df['count_docs']:\n",
    "    try:\n",
    "        counts.append(count[0].split(' ')[0])\n",
    "    except IndexError:\n",
    "        counts.append(0)\n",
    "        \n",
    "    try:\n",
    "        case.append(count[0].split(' ')[1])\n",
    "    except IndexError:\n",
    "        case.append('')\n",
    "\n",
    "df['counts'] = counts\n",
    "df['counts'] = [str(count).replace(',', '') for count in df['counts']]\n",
    "df['counts'] = [str(count).replace('.', '') for count in df['counts']]\n",
    "df['case'] = case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing Locations\n",
    "\n",
    "ph_loc = []\n",
    "for loc in df['PH_Loc']:\n",
    "    try:\n",
    "        ph_loc.append(loc[0])\n",
    "    except IndexError:\n",
    "        ph_loc.append('')\n",
    "df['Loc'] = ph_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing confirmed counts\n",
    "\n",
    "count_fixer = []\n",
    "for index, row in df.iterrows():\n",
    "    if (row['status'] == 'confirmed') & (row['case'] != 'confirmed'):\n",
    "        count_fixer.append(1)\n",
    "    else:\n",
    "        count_fixer.append(row['counts'])\n",
    "\n",
    "df['counts'] = count_fixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing for CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)\n",
    "df.to_csv('rappler_parsed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rappler_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(df):\n",
    "    print(df.info())\n",
    "    \n",
    "    # Get min/max/mean values\n",
    "    dfa = pd.pivot_table(df, values = 'counts', index=['date', 'Loc'], columns='status', aggfunc=[min, max, np.mean, stats.mode])\n",
    "    \n",
    "    # Remove multi-index\n",
    "    dfa.columns = [\"_\".join(pair) for pair in dfa.columns]\n",
    "    dfa = dfa.reset_index()\n",
    "    \n",
    "    # Replace 0 with np.nan to forward fill null values\n",
    "    dfa = dfa.replace(0, np.nan)\n",
    "    \n",
    "    # Forward filling needs to be by area\n",
    "    places = list(df['Loc'].unique())\n",
    "    \n",
    "    global dfb\n",
    "    dfb = pd.DataFrame()\n",
    "    for place in places:\n",
    "        df_temp = dfa[dfa['Loc'] == place].fillna(method='ffill')\n",
    "        dfb = dfb.append(df_temp)\n",
    "    return dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68 entries, 0 to 67\n",
      "Data columns (total 13 columns):\n",
      "source_id     68 non-null int64\n",
      "date          68 non-null object\n",
      "category      68 non-null object\n",
      "title         68 non-null object\n",
      "author        68 non-null object\n",
      "text          68 non-null object\n",
      "LDA_Topics    68 non-null object\n",
      "count_docs    68 non-null object\n",
      "PH_Loc        68 non-null object\n",
      "status        31 non-null object\n",
      "counts        68 non-null int64\n",
      "case          17 non-null object\n",
      "Loc           30 non-null object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 7.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "res = parse(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Loc</th>\n",
       "      <th>min_confirmed</th>\n",
       "      <th>min_suspected</th>\n",
       "      <th>max_confirmed</th>\n",
       "      <th>max_suspected</th>\n",
       "      <th>mean_confirmed</th>\n",
       "      <th>mean_suspected</th>\n",
       "      <th>mode_confirmed</th>\n",
       "      <th>mode_suspected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>cebu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>555.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([0], [2])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>cebu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([14], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>cebu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([0], [2])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>cebu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>([1], [2])</td>\n",
       "      <td>([100], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>cebu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>([1], [1])</td>\n",
       "      <td>([100], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>cebu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>([1], [1])</td>\n",
       "      <td>([100], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>aklan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([3], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>aklan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([80], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>aklan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([11], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>aklan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>([1], [1])</td>\n",
       "      <td>([11], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>aklan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>([1], [1])</td>\n",
       "      <td>([0], [1])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>camiguin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([106], [1])</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date       Loc  min_confirmed  min_suspected  max_confirmed  \\\n",
       "1   2020-01-23      cebu            NaN            NaN            NaN   \n",
       "2   2020-01-24      cebu            NaN           14.0            NaN   \n",
       "7   2020-01-30      cebu            NaN           14.0            NaN   \n",
       "9   2020-01-31      cebu            1.0          100.0            1.0   \n",
       "10  2020-02-02      cebu            1.0          100.0            1.0   \n",
       "11  2020-02-05      cebu            1.0          100.0            1.0   \n",
       "0   2020-01-23     aklan            NaN            3.0            NaN   \n",
       "3   2020-01-25     aklan            NaN           80.0            NaN   \n",
       "5   2020-01-28     aklan            NaN           11.0            NaN   \n",
       "6   2020-01-30     aklan            1.0           11.0            1.0   \n",
       "8   2020-01-31     aklan            1.0           11.0            1.0   \n",
       "4   2020-01-27  camiguin            NaN          106.0            NaN   \n",
       "\n",
       "    max_suspected  mean_confirmed  mean_suspected mode_confirmed  \\\n",
       "1           555.0             NaN           185.0            NaN   \n",
       "2            14.0             NaN            14.0            NaN   \n",
       "7            14.0             NaN            14.0            NaN   \n",
       "9           100.0             1.0           100.0     ([1], [2])   \n",
       "10          100.0             1.0           100.0     ([1], [1])   \n",
       "11          100.0             1.0           100.0     ([1], [1])   \n",
       "0             3.0             NaN             3.0            NaN   \n",
       "3            80.0             NaN            80.0            NaN   \n",
       "5            11.0             NaN            11.0            NaN   \n",
       "6            11.0             1.0            11.0     ([1], [1])   \n",
       "8            11.0             1.0            11.0     ([1], [1])   \n",
       "4           106.0             NaN           106.0            NaN   \n",
       "\n",
       "   mode_suspected  \n",
       "1      ([0], [2])  \n",
       "2     ([14], [1])  \n",
       "7      ([0], [2])  \n",
       "9    ([100], [1])  \n",
       "10   ([100], [1])  \n",
       "11   ([100], [1])  \n",
       "0      ([3], [1])  \n",
       "3     ([80], [1])  \n",
       "5     ([11], [1])  \n",
       "6     ([11], [1])  \n",
       "8      ([0], [1])  \n",
       "4    ([106], [1])  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[['date','Loc', 'min_suspected','min_confirmed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov.to_file(\"provinces.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov = gpd.read_file('provinces.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(res, prov, left_on = 'Loc', right_on = 'Pro_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['date','Loc','min_suspected','min_confirmed','long','lat']]\n",
    "df.columns = (['Date','Location','Suspected','Confirmed','Longitude','Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = [datetime.datetime.strptime(str(date), '%Y-%m-%d').strftime('%Y-%m-%dT%H:%M:%S.%f') for date in df['Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ncov_parsed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
