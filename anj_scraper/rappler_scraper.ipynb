def rappler_scrape():
  import urllib
  from bs4 import BeautifulSoup
  import pandas as pd
  import time
  from urllib.request import Request, urlopen
  
  rappler_ncov_news = []
  page_link = '/previous-articles?filterMeta=coronavirus%20philippine%20updates'
  user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0)'
  
  while page_link != False:
    req = Request('https://www.rappler.com' + page_link, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
        'Accept-Encoding': 'none',
        'Accept-Language': 'en-US,en;q=0.8',
        'Connection': 'keep-alive'})


    content = urlopen(req).read()

    print('reading url')
    soup = BeautifulSoup(content)

    print('reading main news section')
    mydivs = soup.find("div", {'id':"article-finder-result"})

    print('reading urls in news section')
    mydivs = mydivs.find_all("div", {'class':"row"})

    rappler_ncov_news += [url.a['href'] for url in mydivs] 
    time.sleep(6)

    try:
        page_link = soup.find("li", {'class':"pagination-next"})['href']
    except:
        page_link = False
